{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "#import lightgbm as lgb\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    prev_word, prev_pos = '', ''\n",
    "    nouns = []\n",
    "    words = []\n",
    "    etc = []\n",
    "    buff = ''\n",
    "    ret = []\n",
    "    condition = False\n",
    "    idx, condidx = 0, 0\n",
    "    \n",
    "    if sentence[:4] == '(광고)':\n",
    "        words.append('(광고)')\n",
    "        nouns.append('(광고)')\n",
    "        sentence = sentence[4:]\n",
    "\n",
    "    for word, pos in mecab.pos(sentence):\n",
    "\n",
    "        if pos in ('SF') or pos[:1] in ('J'):\n",
    "            # 은/는/이/가/. 등\n",
    "            prev_word, prev_pos = '', ''\n",
    "            continue\n",
    "\n",
    "        if pos[:1] == 'N' and prev_pos[:1] == 'S':\n",
    "            # 1억, 1천만원 등\n",
    "            word = re.sub('[억천만]', '', word)\n",
    "            if len(word) == 0:\n",
    "                continue\n",
    "\n",
    "        if pos[:1] == 'S':\n",
    "            word = re.sub('[(:)]', '', word)\n",
    "            word = re.sub('[0-9.X]+', 'X', word)\n",
    "\n",
    "        # 조건절 판단\n",
    "        if prev_pos[-3:] == 'ETM' and word in ['분','고객','개인','사업자','이','대상','당신','직원','VIP', '자','분도','전문직','신용자','외국인','본인','임직원','부모','회계사','귀하','투자자','분과']:\n",
    "            condition = True  \n",
    "        elif pos[:1] == 'N':\n",
    "            if word[:2] in ('경우', '필요', '라면', '다면'): # 경우, 필요 단어 등장 시 조건절로 판단\n",
    "                condition = True\n",
    "        elif pos[-2:] == 'EC':\n",
    "            # 여도, 라도, 라면, 면\n",
    "            if word[-1:] in ('면', '도') or word[-2:] in ('도록'):\n",
    "                condition = True\n",
    "\n",
    "        if pos[:1] == 'E':\n",
    "            buff += word\n",
    "        #elif pos[:1] == 'N' and prev_pos[:1] == 'N' and len(word) == 1:\n",
    "        #    buff += word\n",
    "        elif pos[:1] == 'S' and prev_pos[:1] == 'S' and word[:1] in 'X0123456789%.-': #('%', '.', '-'):\n",
    "            if buff[-1:] in ('시', '원', '%'):\n",
    "                words.append(buff)\n",
    "                buff = ''\n",
    "            buff += word\n",
    "            #buff += re.sub('[0-9.X]+', 'X', word)\n",
    "            #buff = re.sub('[0-9.X]+', 'X', buff)\n",
    "        elif pos[:1] == 'N' and prev_pos[:1] == 'S' and len(buff) > 0 and word in ('시', '분', '초', '시간'):\n",
    "            buff = re.sub('시', '', buff)\n",
    "            buff += '시'\n",
    "        elif pos[:1] == 'N' and prev_pos[:1] == 'S' and len(buff) > 0 and word in ('년', '월', '일', '개월'):\n",
    "            buff = re.sub('일', '', buff)\n",
    "            buff += '일'\n",
    "        elif pos[:1] == 'N' and prev_pos[:1] == 'S' and len(buff) > 0 and word in ('원'):\n",
    "            buff = re.sub('원', '', buff)\n",
    "            buff += '원'\n",
    "        elif pos[:1] == 'N' and prev_pos[:1] == 'S' and len(buff) > 0 and word in ('건', '배', 'pt'):\n",
    "            buff += word\n",
    "        else:\n",
    "            if len(buff) > 0:\n",
    "                buff = re.sub('[0-9.X]+', 'X', buff)\n",
    "                buff = re.sub('[-]+', '-', buff)\n",
    "                buff = re.sub('[X]+', 'X', buff)\n",
    "                if True: #buff != 'X':\n",
    "                    words.append(buff)\n",
    "                    # M, N 경우만 저장해보자\n",
    "                    if prev_pos[:1] in ('N'):  # 'M', \n",
    "                        nouns.append(buff)\n",
    "            buff = word\n",
    "            prev_word, prev_pos = word, pos\n",
    "\n",
    "        if pos[-2:] in ('EF'): # and word[0][-1:] in ['요', '다']:\n",
    "            # words가 꼭 명사를 뜻하는 게 아니고 특색있는 키워드 모두를 포함\n",
    "            if len(buff) > 0:\n",
    "                words.append(buff)\n",
    "            buff = ''\n",
    "            # 문장의 끝을 구분\n",
    "            words.append('(절취선)')\n",
    "            ret.append([word, condition, words, nouns])\n",
    "            idx += 1\n",
    "            if condition: condidx += 1\n",
    "            nouns = []\n",
    "            words = []\n",
    "            condition = False\n",
    "    if len(buff) > 0:\n",
    "        words.append(buff)\n",
    "    if len(words) > 0:\n",
    "        ret.append([word, condition, words, nouns])\n",
    "        #ret.append([word, condition, nouns])\n",
    "        idx += 1\n",
    "        if condition: condidx += 1\n",
    "    etc.append(condidx / idx)\n",
    "    return ret, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-08 22:36:17.106886 0\n",
      "2020-01-08 22:37:26.995470 100000\n",
      "2020-01-08 22:39:13.700527 200000\n"
     ]
    }
   ],
   "source": [
    "smishings = []\n",
    "normals = []\n",
    "idx = 0\n",
    "for idx, item in df.iterrows():\n",
    "    #print(item.text)\n",
    "    splited, etc = parse_sentence(item.text)\n",
    "    etc.append(np.log(len(item.text))/8)  # 문장 전체의 길이\n",
    "    etc.append(np.log(len(splited))/4)  # 문장의 개수\n",
    "    splited.append(etc)\n",
    "    splited.append(item.id)\n",
    "    #splited = m.parse(item.text).split()\n",
    "    if item.smishing == 1:\n",
    "        smishings.append(splited)\n",
    "    else:\n",
    "        normals.append(splited)\n",
    "    if idx % 100000 == 0:\n",
    "        print(datetime.now(), idx)\n",
    "    idx += 1\n",
    "#     if idx > 20000:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected normal-like words count: 223\n",
      "selected smishing-like words count: 1698\n"
     ]
    }
   ],
   "source": [
    "# 스미싱에만 등장하는 문자들 : smish_target_words\n",
    "NORM_THRESHOLD, SMISH_THRESHOLD = 1000, 2\n",
    "norm_dict = {}\n",
    "for item in [x for normal in normals for row in normal if type(row) == list and len(row) > 3 for x in row[3]]:\n",
    "    if item in norm_dict:\n",
    "        norm_dict[item] += 1\n",
    "    else:\n",
    "        norm_dict[item] = 1\n",
    "norm_filter_words = {x:norm_dict.get(x) for x in norm_dict if norm_dict.get(x) > NORM_THRESHOLD}\n",
    "\n",
    "smish_dict = {}\n",
    "for item in [x for smishing in smishings for row in smishing if type(row) == list and len(row) > 3 for x in row[3]]:\n",
    "    if item in smish_dict:\n",
    "        smish_dict[item] += 1\n",
    "    else:\n",
    "        smish_dict[item] = 1\n",
    "smish_filter_words = {x:smish_dict.get(x) for x in smish_dict if smish_dict.get(x) > SMISH_THRESHOLD}\n",
    "        \n",
    "word_only_in_smishings = [word for word in smish_dict if word not in norm_filter_words]\n",
    "word_only_in_normals = [word for word in norm_dict if word not in smish_filter_words]\n",
    "\n",
    "norm_target_words = {x:norm_dict.get(x) for x in word_only_in_normals if norm_dict.get(x) > NORM_THRESHOLD}\n",
    "smish_target_words = {x:smish_dict.get(x) for x in word_only_in_smishings if smish_dict.get(x) > SMISH_THRESHOLD}\n",
    "print('selected normal-like words count:', len(norm_target_words))\n",
    "print('selected smishing-like words count:', len(smish_target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "smishing_word_cnt = []\n",
    "normal_word_cnt = []\n",
    "for smishing in smishings:\n",
    "    notarget_word_list = [x for row in smishing[:-2] for x in row[3] if x in norm_target_words]\n",
    "    target_word_list = [x for row in smishing[:-2] for x in row[3] if x in smish_target_words]\n",
    "    smishing.insert(-2, notarget_word_list)\n",
    "    smishing.insert(-2, target_word_list)\n",
    "    smishing_word_cnt.append(len(target_word_list))\n",
    "for normal in normals:\n",
    "    notarget_word_list = [x for row in normal[:-2] for x in row[3] if x in norm_target_words]\n",
    "    target_word_list = [x for row in normal[:-2] for x in row[3] if x in smish_target_words]\n",
    "    normal.insert(-2, notarget_word_list)\n",
    "    normal.insert(-2, target_word_list)\n",
    "    normal_word_cnt.append(len(target_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9d143ccdd0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.subplots(ncols=2)\n",
    "plt.plot(smishing_word_cnt)\n",
    "plt.plot(normal_word_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['십니까', False, ['안녕', '하십니까', '(절취선)'], ['안녕']]\n",
      "['입니다', False, ['고객', '님', 'X', '은행', '입니다', '(절취선)'], ['고객', '은행']]\n",
      "['입니다', False, ['금일', '납부', '하셔야', '할', '금액', 'X원', '입니다', '(절취선)'], ['금일', '납부', '금액']]\n",
      "['합니다', False, ['감사', '합니다', '(절취선)'], ['감사']]\n",
      "['으십시오', False, ['새해', '복', '많이', '받으십시오', '(절취선)'], ['새해', '복']]\n",
      "['올림', False, ['X', '은행', '옥포', 'X', '올림'], ['은행', '옥포']]\n",
      "['옥포']\n",
      "[]\n",
      "[0.0, 0.5493061443340549, 0.44793986730701374]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x) for x in normals[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordCounter():\n",
    "    def __init__(self):\n",
    "        self.word_bank = {}\n",
    "    \n",
    "    def add_bucket(self, bucket_name):\n",
    "        self.word_bank[bucket_name] = {}\n",
    "        \n",
    "    def add_list(self, bucket_name, word_list:[]):\n",
    "        bucket = self.word_bank[bucket_name]\n",
    "        for keyword in word_list:\n",
    "            if keyword in bucket:\n",
    "                bucket[keyword] += 1\n",
    "            else:\n",
    "                bucket[keyword] = 1\n",
    "                \n",
    "    def get_counts(self, bucket_name):\n",
    "        return self.word_bank[bucket_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wordCounter()\n",
    "wc.add_bucket('conditioned')\n",
    "wc.add_bucket('general')\n",
    "wc.add_bucket('whole')\n",
    "wc.add_bucket('notargets')\n",
    "wc.add_bucket('targets')\n",
    "\n",
    "LEN_OF_OTHERS = 4\n",
    "\n",
    "for normal in normals:\n",
    "    for item in normal[:-LEN_OF_OTHERS]:\n",
    "        wc.add_list('whole', item[2])\n",
    "        if item[1] == True:\n",
    "            wc.add_list('conditioned', item[3])\n",
    "        else:\n",
    "            wc.add_list('general', item[3])\n",
    "    wc.add_list('notargets', normal[-4])\n",
    "    wc.add_list('targets', normal[-3])\n",
    "\n",
    "for smishing in smishings:\n",
    "    for item in smishing[:-LEN_OF_OTHERS]:\n",
    "        wc.add_list('whole', item[2])\n",
    "        if item[1] == True:\n",
    "            wc.add_list('conditioned', item[3])\n",
    "        else:\n",
    "            wc.add_list('general', item[3])\n",
    "    wc.add_list('notargets', smishing[-4])\n",
    "    wc.add_list('targets', smishing[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned_df = pd.Series(wc.get_counts('conditioned'))\n",
    "general_df = pd.Series(wc.get_counts('general'))\n",
    "whole_df = pd.Series(wc.get_counts('whole'))\n",
    "notargets_df = pd.Series(wc.get_counts('notargets'))\n",
    "targets_df = pd.Series(wc.get_counts('targets'))\n",
    "\n",
    "# 글자가 2개 이상인 것만 모아보자\n",
    "conditioned_df = conditioned_df[conditioned_df.index.map(len) > 1]  # M 만 포함시켰으므로.. 길이 2이상인것만\n",
    "general_df = general_df[general_df.index.map(len) > 1]  # M 만 포함시켰으므로.. 길이 2이상인것만\n",
    "# targets_df 일단 생략\n",
    "\n",
    "# 3번 이상 출현한 것만 모아보자\n",
    "#conditioned_df = conditioned_df[conditioned_df.sort_values() > 2]\n",
    "#general_df = general_df[general_df.sort_values() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_dic = general_df[general_df > 1].sort_values(ascending=False)\n",
    "general_dic = general_dic.reset_index()['index'].to_dict()\n",
    "general_vocab = {general_dic.get(k):k for k in general_dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned_dic = conditioned_df[conditioned_df > 1].sort_values(ascending=False)\n",
    "conditioned_dic = conditioned_dic.reset_index()['index'].to_dict()\n",
    "conditioned_vocab = {conditioned_dic.get(k):k for k in conditioned_dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dic = whole_df[whole_df > 1].sort_values(ascending=False)\n",
    "whole_dic = whole_dic.reset_index()['index'].to_dict()\n",
    "whole_vocab = {whole_dic.get(k):k for k in whole_dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "notargets_dic = notargets_df[notargets_df > 1].sort_values(ascending=False)\n",
    "notargets_dic = notargets_dic.reset_index()['index'].to_dict()\n",
    "notargets_vocab = {notargets_dic.get(k):k for k in notargets_dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_dic = targets_df[targets_df > 1].sort_values(ascending=False)\n",
    "targets_dic = targets_dic.reset_index()['index'].to_dict()\n",
    "targets_vocab = {targets_dic.get(k):k for k in targets_dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocab (conditioned, general, whole, targets): 9070 15877 38696 1698\n"
     ]
    }
   ],
   "source": [
    "# smishing cases; length of vocab when it's out of vocab\n",
    "tidf_conditioned = []\n",
    "tidf_general = []\n",
    "tidf_whole = []\n",
    "tidf_notargets = []\n",
    "tidf_targets = []\n",
    "tidf_etc = []\n",
    "\n",
    "cond_vocabsize = len(conditioned_vocab)\n",
    "gen_vocabsize = len(general_vocab)\n",
    "whole_vocabsize = len(whole_vocab)\n",
    "notargets_vocabsize = len(notargets_vocab)\n",
    "targets_vocabsize = len(targets_vocab)\n",
    "\n",
    "print('size of vocab (conditioned, general, whole, targets):', cond_vocabsize, gen_vocabsize, whole_vocabsize, targets_vocabsize)\n",
    "\n",
    "# 문장별 bag of word를 제외한 항목의 수 : notargets, targets, etc, idx\n",
    "LEN_OF_OTHERS = 4\n",
    "for msg in normals:\n",
    "    tidf_conditioned.append([conditioned_vocab.get(x) if x in conditioned_vocab else cond_vocabsize for row in msg[:-LEN_OF_OTHERS] if row[1] == True for x in row[3]])\n",
    "    tidf_general.append([general_vocab.get(x) if x in general_vocab else gen_vocabsize for row in msg[:-LEN_OF_OTHERS] if row[1] == False for x in row[3]])\n",
    "    tidf_whole.append([whole_vocab.get(x) if x in whole_vocab else whole_vocabsize for row in msg[:-LEN_OF_OTHERS] for x in row[2]])\n",
    "    tidf_notargets.append([notargets_vocab.get(x) if x in notargets_vocab else notargets_vocabsize for x in msg[-4]])\n",
    "    tidf_targets.append([targets_vocab.get(x) if x in targets_vocab else targets_vocabsize for x in msg[-3]])\n",
    "    tidf_etc.append(msg[-2])\n",
    "for msg in smishings:\n",
    "    tidf_conditioned.append([conditioned_vocab.get(x) if x in conditioned_vocab else cond_vocabsize for row in msg[:-LEN_OF_OTHERS] if row[1] == True for x in row[3]])\n",
    "    tidf_general.append([general_vocab.get(x) if x in general_vocab else gen_vocabsize for row in msg[:-LEN_OF_OTHERS] if row[1] == False for x in row[3]])\n",
    "    tidf_whole.append([whole_vocab.get(x) if x in whole_vocab else whole_vocabsize for row in msg[:-LEN_OF_OTHERS] for x in row[2]])\n",
    "    tidf_notargets.append([notargets_vocab.get(x) if x in notargets_vocab else notargets_vocabsize for x in msg[-4]])\n",
    "    tidf_targets.append([targets_vocab.get(x) if x in targets_vocab else targets_vocabsize for x in msg[-3]])\n",
    "    tidf_etc.append(msg[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295945, 295945, 295945, 295945)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tidf_conditioned), len(tidf_general), len(tidf_etc), len(tidf_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditioned vector minimum size: 260\n",
      "general vector minimum size: 333\n",
      "whole vector minimum size: 508\n",
      "notarget vector minimum size: 43\n",
      "target vector minimum size: 98\n"
     ]
    }
   ],
   "source": [
    "convec_size = max([len(x) for x in tidf_conditioned])\n",
    "genvec_size = max([len(x) for x in tidf_general])\n",
    "whlvec_size = max([len(x) for x in tidf_whole])\n",
    "notgtvec_size = max([len(x) for x in tidf_notargets])\n",
    "tgtvec_size = max([len(x) for x in tidf_targets])\n",
    "\n",
    "# 문장 내 인식한 단어의 개수의 최대값 (문장의 길이가 길수록 크겠지..)\n",
    "print('conditioned vector minimum size:', convec_size)\n",
    "print('general vector minimum size:', genvec_size)\n",
    "print('whole vector minimum size:', whlvec_size)\n",
    "print('notarget vector minimum size:', notgtvec_size)\n",
    "print('target vector minimum size:', tgtvec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용, 훈련검증용, 결과검증용 분리\n",
    "tidf_conditioned = np.array(tidf_conditioned)\n",
    "tidf_general = np.array(tidf_general)\n",
    "tidf_whole = np.array(tidf_whole)\n",
    "tidf_notargets = np.array(tidf_notargets)\n",
    "tidf_targets = np.array(tidf_targets)\n",
    "tidf_etc = np.array(tidf_etc)\n",
    "tidf_label = np.array([1 if x >= len(normals) else 0 for x in range(tidf_conditioned.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, tidf_cond_train, tidf_gen_train, tidf_whole_train, tidf_notargets_train, tidf_targets_train, tidf_etc_train, tidf_label_train, normal_size=160, smishing_size=160, convec_size=350, genvec_size=470, whlvec_size=500, notgtvec_size=100, tgtvec_size=100):\n",
    "        self.convec_size = convec_size  # including 0 paddings\n",
    "        self.genvec_size = genvec_size  # including 0 paddings\n",
    "        self.whlvec_size = whlvec_size  # including 0 paddings\n",
    "        self.notgtvec_size = notgtvec_size  # including 0 paddings\n",
    "        self.tgtvec_size = tgtvec_size  # including 0 paddings\n",
    "        self.tidf_cond_train_normal = pad_sequences(tidf_cond_train[tidf_label_train == 0], maxlen=self.convec_size, padding='pre')\n",
    "        self.tidf_gen_train_normal = pad_sequences(tidf_gen_train[tidf_label_train == 0], maxlen=self.genvec_size, padding='pre')\n",
    "        self.tidf_whole_train_normal = pad_sequences(tidf_whole_train[tidf_label_train == 0], maxlen=self.whlvec_size, padding='pre')\n",
    "        self.tidf_targets_train_normal = pad_sequences(tidf_targets_train[tidf_label_train == 0], maxlen=self.tgtvec_size, padding='pre')\n",
    "        self.tidf_notargets_train_normal = pad_sequences(tidf_notargets_train[tidf_label_train == 0], maxlen=self.notgtvec_size, padding='pre')\n",
    "        self.tidf_etc_train_normal = tidf_etc_train[tidf_label_train == 0]\n",
    "        self.tidf_cond_train_smishing = pad_sequences(tidf_cond_train[tidf_label_train == 1], maxlen=self.convec_size, padding='pre')\n",
    "        self.tidf_gen_train_smishing = pad_sequences(tidf_gen_train[tidf_label_train == 1], maxlen=self.genvec_size, padding='pre')\n",
    "        self.tidf_whole_train_smishing = pad_sequences(tidf_whole_train[tidf_label_train == 1], maxlen=self.whlvec_size, padding='pre')\n",
    "        self.tidf_notargets_train_smishing = pad_sequences(tidf_notargets_train[tidf_label_train == 1], maxlen=self.notgtvec_size, padding='pre')\n",
    "        self.tidf_targets_train_smishing = pad_sequences(tidf_targets_train[tidf_label_train == 1], maxlen=self.tgtvec_size, padding='pre')\n",
    "        self.tidf_etc_train_smishing = tidf_etc_train[tidf_label_train == 1]\n",
    "        self.sample_size_normal = normal_size\n",
    "        self.sample_size_smishing = smishing_size\n",
    "        self.train_size_normal = len(self.tidf_cond_train_normal)\n",
    "        self.train_size_smishing = len(self.tidf_cond_train_smishing)\n",
    "#         self.sample_size = normal_size + smishing_size\n",
    "#         self.num_batch = int(np.ceil(len(self.tidf_cond_train_normal) // self.sample_size / 10))\n",
    "        self.num_batch_normal = int(np.ceil(self.train_size_normal / self.sample_size_normal))\n",
    "        self.num_batch_smishing = int(np.ceil(self.train_size_smishing / self.sample_size_smishing))\n",
    "        self.normal_train_index = np.array(range(self.train_size_normal))\n",
    "        self.smishing_train_index = np.array(range(self.train_size_smishing))\n",
    "        print(f\"num_batch_normal: {self.num_batch_normal}, num_batch_smishing: {self.num_batch_smishing}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(self.num_batch_normal, self.num_batch_smishing)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # sequencial sampling\n",
    "        normal_idx, smishing_idx = index % self.num_batch_normal, index % self.num_batch_smishing\n",
    "        #print(f\"normal_idx: {normal_idx}, smishing_idx: {smishing_idx}\")\n",
    "        normal_sampling = self.normal_train_index // self.sample_size_normal == normal_idx\n",
    "#         normal_sampling = np.random.choice(len(self.tidf_cond_train_normal), self.normal_sample_size)\n",
    "        normal_cond_sample = self.tidf_cond_train_normal[normal_sampling]\n",
    "        normal_gen_sample = self.tidf_gen_train_normal[normal_sampling]\n",
    "        normal_whole_sample = self.tidf_whole_train_normal[normal_sampling]\n",
    "        normal_notargets_sample = self.tidf_notargets_train_normal[normal_sampling]\n",
    "        normal_targets_sample = self.tidf_targets_train_normal[normal_sampling]\n",
    "        normal_etc_sample = self.tidf_etc_train_normal[normal_sampling]\n",
    "        smishing_sampling = self.smishing_train_index // self.sample_size_smishing == smishing_idx\n",
    "#         smishing_sampling = np.random.choice(len(self.tidf_cond_train_smishing), self.smishing_sample_size)\n",
    "        smishing_cond_sample = self.tidf_cond_train_smishing[smishing_sampling]\n",
    "        smishing_gen_sample = self.tidf_gen_train_smishing[smishing_sampling]\n",
    "        smishing_whole_sample = self.tidf_whole_train_smishing[smishing_sampling]\n",
    "        smishing_notargets_sample = self.tidf_notargets_train_smishing[smishing_sampling]\n",
    "        smishing_targets_sample = self.tidf_targets_train_smishing[smishing_sampling]\n",
    "        smishing_etc_sample = self.tidf_etc_train_smishing[smishing_sampling]\n",
    "        sample_cond_train = np.concatenate([normal_cond_sample, smishing_cond_sample])\n",
    "        sample_gen_train = np.concatenate([normal_gen_sample, smishing_gen_sample])\n",
    "        sample_whole_train = np.concatenate([normal_whole_sample, smishing_whole_sample])\n",
    "        sample_notargets_train = np.concatenate([normal_notargets_sample, smishing_notargets_sample])\n",
    "        sample_targets_train = np.concatenate([normal_targets_sample, smishing_targets_sample])\n",
    "        sample_etc_train = np.concatenate([normal_etc_sample, smishing_etc_sample])\n",
    "        sample_label_train = [1 if x >= len(normal_cond_sample) else 0 for x in range(len(sample_cond_train))]\n",
    "        return [sample_cond_train, sample_gen_train, sample_whole_train, sample_notargets_train, sample_targets_train, sample_etc_train], sample_label_train\n",
    "#        return [sample_gen_train, sample_etc_train], sample_label_train\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((177533,), (59281,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_RATIO, VALID_RATIO = 0.6, 0.8\n",
    "np.random.seed(0)\n",
    "split = np.random.rand(tidf_conditioned.shape[0])\n",
    "tidf_cond_train = tidf_conditioned[split < TRAIN_RATIO]\n",
    "tidf_cond_valid = tidf_conditioned[(split >= TRAIN_RATIO) & (split < VALID_RATIO)]\n",
    "tidf_cond_test = tidf_conditioned[split >= VALID_RATIO]\n",
    "\n",
    "tidf_gen_train = tidf_general[split < TRAIN_RATIO]\n",
    "tidf_gen_valid = tidf_general[(split >= TRAIN_RATIO) & (split < VALID_RATIO)]\n",
    "tidf_gen_test = tidf_general[split >= VALID_RATIO]\n",
    "\n",
    "tidf_whole_train = tidf_whole[split < TRAIN_RATIO]\n",
    "tidf_whole_valid = tidf_whole[(split >= TRAIN_RATIO) & (split < VALID_RATIO)]\n",
    "tidf_whole_test = tidf_whole[split >= VALID_RATIO]\n",
    "\n",
    "tidf_notargets_train = tidf_notargets[split < TRAIN_RATIO]\n",
    "tidf_notargets_valid = tidf_notargets[(split >= TRAIN_RATIO) & (split < VALID_RATIO)]\n",
    "tidf_notargets_test = tidf_notargets[split >= VALID_RATIO]\n",
    "\n",
    "tidf_targets_train = tidf_targets[split < TRAIN_RATIO]\n",
    "tidf_targets_valid = tidf_targets[(split >= TRAIN_RATIO) & (split < VALID_RATIO)]\n",
    "tidf_targets_test = tidf_targets[split >= VALID_RATIO]\n",
    "\n",
    "tidf_etc_train = tidf_etc[split < TRAIN_RATIO]\n",
    "tidf_etc_valid = tidf_etc[(split >= TRAIN_RATIO) & (split < VALID_RATIO)]\n",
    "tidf_etc_test = tidf_etc[split >= VALID_RATIO]\n",
    "\n",
    "tidf_label_train = tidf_label[split < TRAIN_RATIO]\n",
    "tidf_label_valid = tidf_label[(split >= TRAIN_RATIO) & (split < VALID_RATIO)]\n",
    "tidf_label_test = tidf_label[split >= VALID_RATIO]\n",
    "\n",
    "tidf_cond_train.shape, tidf_cond_valid.shape #, tidf_cond_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xikizima/.conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xikizima/.conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xikizima/.conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xikizima/.conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xikizima/.conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xikizima/.conda/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xikizima/.conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xikizima/.conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xikizima/.conda/lib/python3.7/site-packages/ipykernel_launcher.py:53: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"Ac...)`\n"
     ]
    }
   ],
   "source": [
    "# conditioned vector minimum size: 260\n",
    "# general vector minimum size: 333\n",
    "# whole vector minimum size: 508\n",
    "# notarget vector minimum size: 43\n",
    "# target vector minimum size: 98\n",
    "\n",
    "#26, 33, 50, 4, 10 * 0.6\n",
    "\n",
    "num_predictive_factors1, num_predictive_factors2, num_predictive_factors3, num_predictive_factors4, num_predictive_factors5 = 16, 20, 30, 2, 6\n",
    "n_cond, n_gen, n_whole, n_notargets, n_targets, n_etc = convec_size, genvec_size, whlvec_size, notgtvec_size, tgtvec_size, len(tidf_etc_train[0])\n",
    "cond_input = keras.layers.Input(shape=(n_cond,), name='Cond')\n",
    "gen_input = keras.layers.Input(shape=(n_gen,), name='General')\n",
    "whole_input = keras.layers.Input(shape=(n_whole,), name='Whole')\n",
    "notargets_input = keras.layers.Input(shape=(n_notargets,), name='NoTargets')\n",
    "targets_input = keras.layers.Input(shape=(n_targets,), name='Targets')\n",
    "etc_input = keras.layers.Input(shape=(n_etc,), name='Etc')\n",
    "\n",
    "cond_vec_mlp = keras.layers.Embedding(cond_vocabsize + 1, num_predictive_factors1, )(cond_input)\n",
    "cond_vec_mlp = keras.layers.Flatten(name='Flattenconds-MLP')(cond_vec_mlp)\n",
    "cond_vec_mlp = keras.layers.Dropout(0.3)(cond_vec_mlp)\n",
    "\n",
    "gen_vec_mlp = keras.layers.Embedding(gen_vocabsize + 1, num_predictive_factors2, )(gen_input)\n",
    "gen_vec_mlp = keras.layers.Flatten(name='Flattengens-MLP')(gen_vec_mlp)\n",
    "gen_vec_mlp = keras.layers.Dropout(0.3)(gen_vec_mlp)\n",
    "\n",
    "whole_vec_mlp = keras.layers.Embedding(whole_vocabsize + 1, num_predictive_factors3, )(whole_input)\n",
    "whole_vec_mlp = keras.layers.Flatten(name='Flattenwhls-MLP')(whole_vec_mlp)\n",
    "whole_vec_mlp = keras.layers.Dropout(0.3)(whole_vec_mlp)\n",
    "\n",
    "notargets_vec_mlp = keras.layers.Embedding(notargets_vocabsize + 1, num_predictive_factors4, )(notargets_input)\n",
    "notargets_vec_mlp = keras.layers.Flatten(name='FlattenNotgts-MLP')(notargets_vec_mlp)\n",
    "notargets_vec_mlp = keras.layers.Dropout(0.3)(notargets_vec_mlp)\n",
    "\n",
    "targets_vec_mlp = keras.layers.Embedding(targets_vocabsize + 1, num_predictive_factors5, )(targets_input)\n",
    "targets_vec_mlp = keras.layers.Flatten(name='Flattentgts-MLP')(targets_vec_mlp)\n",
    "targets_vec_mlp = keras.layers.Dropout(0.3)(targets_vec_mlp)\n",
    "\n",
    "concat_mlp = keras.layers.merge.concatenate([cond_vec_mlp, gen_vec_mlp, whole_vec_mlp, notargets_vec_mlp, targets_vec_mlp, etc_input])\n",
    "concat_dropout_mlp = keras.layers.Dropout(0.3)(concat_mlp)\n",
    "\n",
    "dense = keras.layers.Dense(28, kernel_regularizer=keras.regularizers.l2(0.0025), name='FullyConnected-1', activation='relu')(concat_dropout_mlp)\n",
    "dropout_1 = keras.layers.Dropout(0.3, name='Dropout-1')(dense)\n",
    "\n",
    "#dense_2 = keras.layers.Dense(DENSE2, kernel_regularizer=keras.regularizers.l2(0.001), name='FullyConnected-2', activation='relu')(dropout_1)\n",
    "#dropout_2 = keras.layers.Dropout(0.3, name='Dropout-2')(dense_2)\n",
    "\n",
    "#dense_3 = keras.layers.Dense(DENSE3, kernel_regularizer=keras.regularizers.l2(0.001), name='FullyConnected-3', activation='relu')(dropout_2)\n",
    "#dropout_3 = keras.layers.Dropout(0.3, name='Dropout-3')(dense_3)\n",
    "\n",
    "dense_4 = keras.layers.Dense(28, kernel_regularizer=keras.regularizers.l2(0.0025), name='FullyConnected-4', activation='relu')(dropout_1)\n",
    "pred_mlp = keras.layers.Dense(1, activation='sigmoid', name='Activation')(dense_4)\n",
    "\n",
    "model = keras.Model(inputs=[cond_input, gen_input, whole_input, notargets_input, targets_input, etc_input ], output=pred_mlp)  #  etc_input\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss='binary_crossentropy', metrics=['binary_crossentropy', 'accuracy'])\n",
    "#model.compile(optimizer=keras.optimizers.Adamax(lr=1e-3), loss='binary_crossentropy', metrics=['binary_crossentropy', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batch_normal: 1039, num_batch_smishing: 71\n",
      "num_batch_normal: 56, num_batch_smishing: 4\n",
      "Epoch 1/100\n",
      "1039/1039 [==============================] - 262s 252ms/step - loss: 0.0082 - binary_crossentropy: 0.0025 - acc: 0.9996 - val_loss: 0.0095 - val_binary_crossentropy: 0.0045 - val_acc: 0.9989\n",
      "Epoch 2/100\n",
      "1039/1039 [==============================] - 263s 253ms/step - loss: 0.0074 - binary_crossentropy: 0.0024 - acc: 0.9997 - val_loss: 0.0107 - val_binary_crossentropy: 0.0059 - val_acc: 0.9985\n",
      "Epoch 3/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0066 - binary_crossentropy: 0.0021 - acc: 0.9997 - val_loss: 0.0107 - val_binary_crossentropy: 0.0061 - val_acc: 0.9986\n",
      "Epoch 4/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0060 - binary_crossentropy: 0.0019 - acc: 0.9997 - val_loss: 0.0085 - val_binary_crossentropy: 0.0048 - val_acc: 0.9988\n",
      "Epoch 5/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0054 - binary_crossentropy: 0.0017 - acc: 0.9998 - val_loss: 0.0080 - val_binary_crossentropy: 0.0044 - val_acc: 0.9989\n",
      "Epoch 6/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0051 - binary_crossentropy: 0.0016 - acc: 0.9998 - val_loss: 0.0082 - val_binary_crossentropy: 0.0048 - val_acc: 0.9987\n",
      "Epoch 7/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0046 - binary_crossentropy: 0.0014 - acc: 0.9998 - val_loss: 0.0068 - val_binary_crossentropy: 0.0037 - val_acc: 0.9991\n",
      "Epoch 8/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0045 - binary_crossentropy: 0.0014 - acc: 0.9998 - val_loss: 0.0069 - val_binary_crossentropy: 0.0041 - val_acc: 0.9989\n",
      "Epoch 9/100\n",
      "1039/1039 [==============================] - 260s 250ms/step - loss: 0.0041 - binary_crossentropy: 0.0013 - acc: 0.9998 - val_loss: 0.0081 - val_binary_crossentropy: 0.0054 - val_acc: 0.9987\n",
      "Epoch 10/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0039 - binary_crossentropy: 0.0012 - acc: 0.9998 - val_loss: 0.0080 - val_binary_crossentropy: 0.0055 - val_acc: 0.9988\n",
      "Epoch 11/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0036 - binary_crossentropy: 0.0011 - acc: 0.9998 - val_loss: 0.0083 - val_binary_crossentropy: 0.0057 - val_acc: 0.9987\n",
      "Epoch 12/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0034 - binary_crossentropy: 9.9892e-04 - acc: 0.9998 - val_loss: 0.0061 - val_binary_crossentropy: 0.0038 - val_acc: 0.9990\n",
      "Epoch 13/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0032 - binary_crossentropy: 9.4617e-04 - acc: 0.9998 - val_loss: 0.0056 - val_binary_crossentropy: 0.0034 - val_acc: 0.9991\n",
      "Epoch 14/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0031 - binary_crossentropy: 9.0904e-04 - acc: 0.9999 - val_loss: 0.0077 - val_binary_crossentropy: 0.0049 - val_acc: 0.9988\n",
      "Epoch 15/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0030 - binary_crossentropy: 8.5758e-04 - acc: 0.9999 - val_loss: 0.0074 - val_binary_crossentropy: 0.0054 - val_acc: 0.9990\n",
      "Epoch 16/100\n",
      "1039/1039 [==============================] - 260s 251ms/step - loss: 0.0028 - binary_crossentropy: 8.3144e-04 - acc: 0.9999 - val_loss: 0.0058 - val_binary_crossentropy: 0.0038 - val_acc: 0.9990\n",
      "Epoch 17/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0028 - binary_crossentropy: 8.1285e-04 - acc: 0.9999 - val_loss: 0.0076 - val_binary_crossentropy: 0.0056 - val_acc: 0.9988\n",
      "Epoch 18/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0026 - binary_crossentropy: 7.4294e-04 - acc: 0.9999 - val_loss: 0.0069 - val_binary_crossentropy: 0.0051 - val_acc: 0.9988\n",
      "Epoch 19/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0026 - binary_crossentropy: 7.4127e-04 - acc: 0.9999 - val_loss: 0.0066 - val_binary_crossentropy: 0.0046 - val_acc: 0.9988\n",
      "Epoch 20/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0024 - binary_crossentropy: 6.7172e-04 - acc: 0.9999 - val_loss: 0.0052 - val_binary_crossentropy: 0.0035 - val_acc: 0.9990\n",
      "Epoch 21/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0024 - binary_crossentropy: 6.4110e-04 - acc: 0.9999 - val_loss: 0.0070 - val_binary_crossentropy: 0.0053 - val_acc: 0.9989\n",
      "Epoch 22/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0023 - binary_crossentropy: 6.8259e-04 - acc: 0.9999 - val_loss: 0.0062 - val_binary_crossentropy: 0.0047 - val_acc: 0.9989\n",
      "Epoch 23/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0022 - binary_crossentropy: 6.0407e-04 - acc: 0.9999 - val_loss: 0.0061 - val_binary_crossentropy: 0.0046 - val_acc: 0.9989\n",
      "Epoch 24/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0022 - binary_crossentropy: 6.2263e-04 - acc: 0.9999 - val_loss: 0.0059 - val_binary_crossentropy: 0.0044 - val_acc: 0.9989\n",
      "Epoch 25/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0021 - binary_crossentropy: 6.0451e-04 - acc: 0.9999 - val_loss: 0.0062 - val_binary_crossentropy: 0.0037 - val_acc: 0.9990\n",
      "Epoch 26/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0021 - binary_crossentropy: 5.6485e-04 - acc: 0.9999 - val_loss: 0.0052 - val_binary_crossentropy: 0.0038 - val_acc: 0.9991\n",
      "Epoch 27/100\n",
      "1039/1039 [==============================] - 262s 252ms/step - loss: 0.0020 - binary_crossentropy: 5.3025e-04 - acc: 0.9999 - val_loss: 0.0071 - val_binary_crossentropy: 0.0056 - val_acc: 0.9988\n",
      "Epoch 28/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0020 - binary_crossentropy: 5.1654e-04 - acc: 0.9999 - val_loss: 0.0059 - val_binary_crossentropy: 0.0046 - val_acc: 0.9989\n",
      "Epoch 29/100\n",
      "1039/1039 [==============================] - 260s 250ms/step - loss: 0.0020 - binary_crossentropy: 5.4239e-04 - acc: 0.9999 - val_loss: 0.0068 - val_binary_crossentropy: 0.0054 - val_acc: 0.9988\n",
      "Epoch 30/100\n",
      "1039/1039 [==============================] - 261s 252ms/step - loss: 0.0019 - binary_crossentropy: 4.9015e-04 - acc: 0.9999 - val_loss: 0.0070 - val_binary_crossentropy: 0.0057 - val_acc: 0.9989\n",
      "Epoch 31/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0019 - binary_crossentropy: 4.7240e-04 - acc: 0.9999 - val_loss: 0.0071 - val_binary_crossentropy: 0.0059 - val_acc: 0.9988\n",
      "Epoch 32/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0018 - binary_crossentropy: 4.8533e-04 - acc: 0.9999 - val_loss: 0.0049 - val_binary_crossentropy: 0.0036 - val_acc: 0.9990\n",
      "Epoch 33/100\n",
      "1039/1039 [==============================] - 260s 250ms/step - loss: 0.0017 - binary_crossentropy: 4.7485e-04 - acc: 0.9999 - val_loss: 0.0064 - val_binary_crossentropy: 0.0043 - val_acc: 0.9988\n",
      "Epoch 34/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0019 - binary_crossentropy: 4.5820e-04 - acc: 0.9999 - val_loss: 0.0067 - val_binary_crossentropy: 0.0055 - val_acc: 0.9987\n",
      "Epoch 35/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0018 - binary_crossentropy: 4.5808e-04 - acc: 0.9999 - val_loss: 0.0048 - val_binary_crossentropy: 0.0037 - val_acc: 0.9990\n",
      "Epoch 36/100\n",
      "1039/1039 [==============================] - 260s 250ms/step - loss: 0.0017 - binary_crossentropy: 4.6477e-04 - acc: 0.9999 - val_loss: 0.0067 - val_binary_crossentropy: 0.0055 - val_acc: 0.9988\n",
      "Epoch 37/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0017 - binary_crossentropy: 3.9708e-04 - acc: 0.9999 - val_loss: 0.0062 - val_binary_crossentropy: 0.0051 - val_acc: 0.9990\n",
      "Epoch 38/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0015 - binary_crossentropy: 4.0641e-04 - acc: 0.9999 - val_loss: 0.0064 - val_binary_crossentropy: 0.0052 - val_acc: 0.9988\n",
      "Epoch 39/100\n",
      "1039/1039 [==============================] - 258s 249ms/step - loss: 0.0016 - binary_crossentropy: 4.0732e-04 - acc: 0.9999 - val_loss: 0.0068 - val_binary_crossentropy: 0.0056 - val_acc: 0.9989\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039/1039 [==============================] - 260s 251ms/step - loss: 0.0017 - binary_crossentropy: 4.1086e-04 - acc: 0.9999 - val_loss: 0.0059 - val_binary_crossentropy: 0.0045 - val_acc: 0.9988\n",
      "Epoch 41/100\n",
      "1039/1039 [==============================] - 258s 248ms/step - loss: 0.0016 - binary_crossentropy: 3.6012e-04 - acc: 0.9999 - val_loss: 0.0063 - val_binary_crossentropy: 0.0051 - val_acc: 0.9988\n",
      "Epoch 42/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0015 - binary_crossentropy: 3.6930e-04 - acc: 0.9999 - val_loss: 0.0056 - val_binary_crossentropy: 0.0047 - val_acc: 0.9988\n",
      "Epoch 43/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0015 - binary_crossentropy: 3.5235e-04 - acc: 0.9999 - val_loss: 0.0065 - val_binary_crossentropy: 0.0047 - val_acc: 0.9988\n",
      "Epoch 44/100\n",
      "1039/1039 [==============================] - 260s 251ms/step - loss: 0.0016 - binary_crossentropy: 3.7419e-04 - acc: 0.9999 - val_loss: 0.0060 - val_binary_crossentropy: 0.0048 - val_acc: 0.9988\n",
      "Epoch 45/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0014 - binary_crossentropy: 3.5665e-04 - acc: 0.9999 - val_loss: 0.0084 - val_binary_crossentropy: 0.0066 - val_acc: 0.9987\n",
      "Epoch 46/100\n",
      "1039/1039 [==============================] - 258s 249ms/step - loss: 0.0015 - binary_crossentropy: 3.6164e-04 - acc: 0.9999 - val_loss: 0.0065 - val_binary_crossentropy: 0.0055 - val_acc: 0.9988\n",
      "Epoch 47/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0015 - binary_crossentropy: 3.3909e-04 - acc: 0.9999 - val_loss: 0.0071 - val_binary_crossentropy: 0.0061 - val_acc: 0.9988\n",
      "Epoch 48/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0014 - binary_crossentropy: 3.1042e-04 - acc: 0.9999 - val_loss: 0.0053 - val_binary_crossentropy: 0.0044 - val_acc: 0.9989\n",
      "Epoch 49/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0014 - binary_crossentropy: 3.2447e-04 - acc: 0.9999 - val_loss: 0.0065 - val_binary_crossentropy: 0.0055 - val_acc: 0.9987\n",
      "Epoch 50/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0015 - binary_crossentropy: 3.2805e-04 - acc: 0.9999 - val_loss: 0.0066 - val_binary_crossentropy: 0.0056 - val_acc: 0.9987\n",
      "Epoch 51/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0014 - binary_crossentropy: 3.2579e-04 - acc: 0.9999 - val_loss: 0.0063 - val_binary_crossentropy: 0.0053 - val_acc: 0.9988\n",
      "Epoch 52/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0013 - binary_crossentropy: 3.0070e-04 - acc: 0.9999 - val_loss: 0.0052 - val_binary_crossentropy: 0.0043 - val_acc: 0.9988\n",
      "Epoch 53/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0014 - binary_crossentropy: 3.0117e-04 - acc: 0.9999 - val_loss: 0.0054 - val_binary_crossentropy: 0.0045 - val_acc: 0.9988\n",
      "Epoch 54/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0014 - binary_crossentropy: 2.9900e-04 - acc: 0.9999 - val_loss: 0.0054 - val_binary_crossentropy: 0.0046 - val_acc: 0.9988\n",
      "Epoch 55/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0013 - binary_crossentropy: 3.0306e-04 - acc: 0.9999 - val_loss: 0.0065 - val_binary_crossentropy: 0.0045 - val_acc: 0.9988\n",
      "Epoch 56/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0013 - binary_crossentropy: 2.7943e-04 - acc: 0.9999 - val_loss: 0.0067 - val_binary_crossentropy: 0.0057 - val_acc: 0.9987\n",
      "Epoch 57/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.7207e-04 - acc: 1.0000 - val_loss: 0.0064 - val_binary_crossentropy: 0.0056 - val_acc: 0.9988\n",
      "Epoch 58/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0013 - binary_crossentropy: 2.8289e-04 - acc: 0.9999 - val_loss: 0.0048 - val_binary_crossentropy: 0.0040 - val_acc: 0.9988\n",
      "Epoch 59/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.6886e-04 - acc: 0.9999 - val_loss: 0.0065 - val_binary_crossentropy: 0.0057 - val_acc: 0.9988\n",
      "Epoch 60/100\n",
      "1039/1039 [==============================] - 258s 249ms/step - loss: 0.0013 - binary_crossentropy: 2.6557e-04 - acc: 0.9999 - val_loss: 0.0067 - val_binary_crossentropy: 0.0057 - val_acc: 0.9988\n",
      "Epoch 61/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.7715e-04 - acc: 0.9999 - val_loss: 0.0053 - val_binary_crossentropy: 0.0043 - val_acc: 0.9988\n",
      "Epoch 62/100\n",
      "1039/1039 [==============================] - 258s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.5531e-04 - acc: 1.0000 - val_loss: 0.0048 - val_binary_crossentropy: 0.0040 - val_acc: 0.9988\n",
      "Epoch 63/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.7168e-04 - acc: 0.9999 - val_loss: 0.0064 - val_binary_crossentropy: 0.0055 - val_acc: 0.9988\n",
      "Epoch 64/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0012 - binary_crossentropy: 2.4045e-04 - acc: 0.9999 - val_loss: 0.0061 - val_binary_crossentropy: 0.0052 - val_acc: 0.9988\n",
      "Epoch 65/100\n",
      "1039/1039 [==============================] - 258s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.3394e-04 - acc: 1.0000 - val_loss: 0.0052 - val_binary_crossentropy: 0.0044 - val_acc: 0.9988\n",
      "Epoch 66/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.5836e-04 - acc: 0.9999 - val_loss: 0.0068 - val_binary_crossentropy: 0.0056 - val_acc: 0.9988\n",
      "Epoch 67/100\n",
      "1039/1039 [==============================] - 258s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.2640e-04 - acc: 1.0000 - val_loss: 0.0064 - val_binary_crossentropy: 0.0055 - val_acc: 0.9989\n",
      "Epoch 68/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0011 - binary_crossentropy: 2.2324e-04 - acc: 1.0000 - val_loss: 0.0063 - val_binary_crossentropy: 0.0053 - val_acc: 0.9987\n",
      "Epoch 69/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0011 - binary_crossentropy: 2.4155e-04 - acc: 1.0000 - val_loss: 0.0057 - val_binary_crossentropy: 0.0050 - val_acc: 0.9988\n",
      "Epoch 70/100\n",
      "1039/1039 [==============================] - 258s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.4048e-04 - acc: 0.9999 - val_loss: 0.0059 - val_binary_crossentropy: 0.0041 - val_acc: 0.9988\n",
      "Epoch 71/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0011 - binary_crossentropy: 2.3479e-04 - acc: 1.0000 - val_loss: 0.0053 - val_binary_crossentropy: 0.0043 - val_acc: 0.9991\n",
      "Epoch 72/100\n",
      "1039/1039 [==============================] - 260s 250ms/step - loss: 0.0011 - binary_crossentropy: 2.2891e-04 - acc: 1.0000 - val_loss: 0.0065 - val_binary_crossentropy: 0.0058 - val_acc: 0.9988\n",
      "Epoch 73/100\n",
      "1039/1039 [==============================] - 260s 251ms/step - loss: 0.0011 - binary_crossentropy: 2.0360e-04 - acc: 1.0000 - val_loss: 0.0063 - val_binary_crossentropy: 0.0056 - val_acc: 0.9988\n",
      "Epoch 74/100\n",
      "1039/1039 [==============================] - 261s 251ms/step - loss: 0.0011 - binary_crossentropy: 2.0833e-04 - acc: 0.9999 - val_loss: 0.0065 - val_binary_crossentropy: 0.0057 - val_acc: 0.9988\n",
      "Epoch 75/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0011 - binary_crossentropy: 2.1574e-04 - acc: 1.0000 - val_loss: 0.0068 - val_binary_crossentropy: 0.0058 - val_acc: 0.9988\n",
      "Epoch 76/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0011 - binary_crossentropy: 2.1560e-04 - acc: 1.0000 - val_loss: 0.0064 - val_binary_crossentropy: 0.0055 - val_acc: 0.9988\n",
      "Epoch 77/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0011 - binary_crossentropy: 1.8908e-04 - acc: 1.0000 - val_loss: 0.0064 - val_binary_crossentropy: 0.0056 - val_acc: 0.9988\n",
      "Epoch 78/100\n",
      "1039/1039 [==============================] - 260s 250ms/step - loss: 0.0010 - binary_crossentropy: 1.9853e-04 - acc: 1.0000 - val_loss: 0.0059 - val_binary_crossentropy: 0.0052 - val_acc: 0.9988\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0012 - binary_crossentropy: 2.0013e-04 - acc: 1.0000 - val_loss: 0.0065 - val_binary_crossentropy: 0.0057 - val_acc: 0.9988\n",
      "Epoch 80/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 9.9190e-04 - binary_crossentropy: 1.8537e-04 - acc: 1.0000 - val_loss: 0.0068 - val_binary_crossentropy: 0.0059 - val_acc: 0.9988\n",
      "Epoch 81/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0010 - binary_crossentropy: 2.0529e-04 - acc: 1.0000 - val_loss: 0.0058 - val_binary_crossentropy: 0.0049 - val_acc: 0.9988\n",
      "Epoch 82/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 0.0011 - binary_crossentropy: 1.9494e-04 - acc: 1.0000 - val_loss: 0.0061 - val_binary_crossentropy: 0.0053 - val_acc: 0.9988\n",
      "Epoch 83/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0010 - binary_crossentropy: 1.9075e-04 - acc: 1.0000 - val_loss: 0.0066 - val_binary_crossentropy: 0.0058 - val_acc: 0.9988\n",
      "Epoch 84/100\n",
      "1039/1039 [==============================] - 258s 249ms/step - loss: 0.0010 - binary_crossentropy: 2.0915e-04 - acc: 1.0000 - val_loss: 0.0066 - val_binary_crossentropy: 0.0058 - val_acc: 0.9988\n",
      "Epoch 85/100\n",
      "1039/1039 [==============================] - 260s 250ms/step - loss: 9.5885e-04 - binary_crossentropy: 1.6487e-04 - acc: 1.0000 - val_loss: 0.0067 - val_binary_crossentropy: 0.0060 - val_acc: 0.9988\n",
      "Epoch 86/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 9.4187e-04 - binary_crossentropy: 1.8559e-04 - acc: 1.0000 - val_loss: 0.0066 - val_binary_crossentropy: 0.0060 - val_acc: 0.9988\n",
      "Epoch 87/100\n",
      "1039/1039 [==============================] - 258s 249ms/step - loss: 9.6740e-04 - binary_crossentropy: 1.8088e-04 - acc: 1.0000 - val_loss: 0.0072 - val_binary_crossentropy: 0.0061 - val_acc: 0.9988\n",
      "Epoch 88/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 9.9526e-04 - binary_crossentropy: 1.6447e-04 - acc: 1.0000 - val_loss: 0.0064 - val_binary_crossentropy: 0.0055 - val_acc: 0.9990\n",
      "Epoch 89/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 9.1094e-04 - binary_crossentropy: 1.6898e-04 - acc: 1.0000 - val_loss: 0.0069 - val_binary_crossentropy: 0.0062 - val_acc: 0.9988\n",
      "Epoch 90/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0010 - binary_crossentropy: 1.6810e-04 - acc: 1.0000 - val_loss: 0.0065 - val_binary_crossentropy: 0.0058 - val_acc: 0.9988\n",
      "Epoch 91/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 9.1358e-04 - binary_crossentropy: 1.7103e-04 - acc: 1.0000 - val_loss: 0.0060 - val_binary_crossentropy: 0.0054 - val_acc: 0.9988\n",
      "Epoch 92/100\n",
      "1039/1039 [==============================] - 259s 250ms/step - loss: 9.4582e-04 - binary_crossentropy: 1.7189e-04 - acc: 1.0000 - val_loss: 0.0087 - val_binary_crossentropy: 0.0059 - val_acc: 0.9988\n",
      "Epoch 93/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 0.0010 - binary_crossentropy: 1.7667e-04 - acc: 1.0000 - val_loss: 0.0053 - val_binary_crossentropy: 0.0047 - val_acc: 0.9988\n",
      "Epoch 94/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 9.7817e-04 - binary_crossentropy: 1.7946e-04 - acc: 1.0000 - val_loss: 0.0063 - val_binary_crossentropy: 0.0057 - val_acc: 0.9988\n",
      "Epoch 95/100\n",
      "1039/1039 [==============================] - 259s 249ms/step - loss: 9.0298e-04 - binary_crossentropy: 1.5410e-04 - acc: 1.0000 - val_loss: 0.0070 - val_binary_crossentropy: 0.0063 - val_acc: 0.9988\n",
      "Epoch 96/100\n",
      " 544/1039 [==============>...............] - ETA: 2:02 - loss: 9.6338e-04 - binary_crossentropy: 1.5646e-04 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3c114931bce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#                               workers=3,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                              \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                              verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/.conda/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_gen = DataGenerator(tidf_cond_train, tidf_gen_train, tidf_whole_train, tidf_notargets_train, tidf_targets_train, tidf_etc_train, tidf_label_train, 160, 160, convec_size, genvec_size, whlvec_size, notgtvec_size, tgtvec_size)\n",
    "valid_gen = DataGenerator(tidf_cond_valid, tidf_gen_valid, tidf_whole_valid, tidf_notargets_valid, tidf_targets_valid, tidf_etc_valid, tidf_label_valid, 1000, 1000, convec_size, genvec_size, whlvec_size, notgtvec_size, tgtvec_size)\n",
    "\n",
    "history = model.fit_generator(generator=train_gen,\n",
    "                             epochs = 100,\n",
    "#                              use_multiprocessing=True,\n",
    "#                               workers=3,\n",
    "                             validation_data=valid_gen,\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, loss_ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "acc_ax = loss_ax.twinx()\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train_loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val_loss')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train_acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val_acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9c8e817f50>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1/9 새벽 출근전\n",
    "data_gen_test = pad_sequences(tidf_gen_test, maxlen=genvec_size, padding='pre')\n",
    "data_cond_test = pad_sequences(tidf_cond_test, maxlen=convec_size, padding='pre')\n",
    "data_whole_test = pad_sequences(tidf_whole_test, maxlen=whlvec_size, padding='pre')\n",
    "data_notargets_test = pad_sequences(tidf_notargets_test, maxlen=notgtvec_size, padding='pre')\n",
    "data_targets_test = pad_sequences(tidf_targets_test, maxlen=tgtvec_size, padding='pre')\n",
    "data_etc_test = tidf_etc_test\n",
    "\n",
    "pred_test = model.predict(x=[data_cond_test, data_gen_test, data_whole_test, data_notargets_test, data_targets_test, data_etc_test], batch_size=160)\n",
    "gap = np.abs(tidf_label_test - pred_test.reshape([-1,]))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc : 0.9999996256597623\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1/9 새벽 출근 전\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(tidf_label_test, pred_test)\n",
    "print('auc :', auc(fpr,tpr))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr,tpr,'-')\n",
    "plt.plot([0,1],[0,1], 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv('./data/public_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-09 06:17:52.665514 0\n"
     ]
    }
   ],
   "source": [
    "unknowns = []\n",
    "\n",
    "idx = 0\n",
    "for idx, item in submit_df.iterrows():\n",
    "    splited, etc = parse_sentence(item.text)\n",
    "    etc.append(np.log(len(item.text))/8)  # 문장 전체의 길이\n",
    "    etc.append(np.log(len(splited))/4)  # 문장의 개수\n",
    "    splited.append(etc)\n",
    "    splited.append(item.id)\n",
    "    unknowns.append(splited)\n",
    "    if idx % 100000 == 0:\n",
    "        print(datetime.now(), idx)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unknown in unknowns:\n",
    "    notarget_word_list = [x for row in unknown[:-2] for x in row[3] if x in norm_target_words]\n",
    "    target_word_list = [x for row in unknown[:-2] for x in row[3] if x in smish_target_words]\n",
    "    unknown.insert(-2, notarget_word_list)\n",
    "    unknown.insert(-2, target_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocab (conditioned, general): 9070 15877\n"
     ]
    }
   ],
   "source": [
    "# smishing cases; length of vocab when it's out of vocab\n",
    "submit_conditioned = []\n",
    "submit_general = []\n",
    "submit_whole = []\n",
    "submit_notargets = []\n",
    "submit_targets = []\n",
    "submit_etc = []\n",
    "\n",
    "cond_vocabsize = len(conditioned_vocab)\n",
    "gen_vocabsize = len(general_vocab)\n",
    "whole_vocabsize = len(whole_vocab)\n",
    "notargets_vocabsize = len(notargets_vocab)\n",
    "targets_vocabsize = len(targets_vocab)\n",
    "\n",
    "print('size of vocab (conditioned, general):', cond_vocabsize, gen_vocabsize)\n",
    "for msg in unknowns:\n",
    "    submit_conditioned.append([conditioned_vocab.get(x) if x in conditioned_vocab else cond_vocabsize for row in msg[:-4] if row[1] == True for x in row[3]])\n",
    "    submit_general.append([general_vocab.get(x) if x in general_vocab else gen_vocabsize for row in msg[:-4] if row[1] == False for x in row[3]])\n",
    "    submit_whole.append([whole_vocab.get(x) if x in whole_vocab else whole_vocabsize for row in msg[:-4] for x in row[2]])\n",
    "    submit_notargets.append([notargets_vocab.get(x) if x in notargets_vocab else notargets_vocabsize for x in msg[-4]])\n",
    "    submit_targets.append([targets_vocab.get(x) if x in targets_vocab else targets_vocabsize for x in msg[-3]])\n",
    "    submit_etc.append(msg[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submit_conditioned = pad_sequences(submit_conditioned, maxlen=convec_size, padding='pre')\n",
    "data_submit_general = pad_sequences(submit_general, maxlen=genvec_size, padding='pre')\n",
    "data_submit_whole = pad_sequences(submit_whole, maxlen=whlvec_size, padding='pre')\n",
    "data_submit_notargets = pad_sequences(submit_notargets, maxlen=notgtvec_size, padding='pre')\n",
    "data_submit_targets = pad_sequences(submit_targets, maxlen=tgtvec_size, padding='pre')\n",
    "data_submit_etc = np.array(submit_etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626/1626 [==============================] - 0s 129us/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = model.predict(x=[data_submit_conditioned, data_submit_general, data_submit_whole, data_submit_notargets, data_submit_targets, data_submit_etc], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y[pred_y > 1.0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df['smishing'] = np.abs(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df[['id', 'smishing']].to_csv('./data/submission_20200109_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반/컨디션 문장의 경우.. 명사만 가지고 만들까?.. 의 컨셉\n",
    "# 거기다 전체 문장은 모든 품사 다 포함 + 여기다 문장 사이에 구분자 추가 (절취선) + mask단어 (XXX) 추가\n",
    "# 여기에 targets라는 smishing에만 주로 등장하는 단어들을 가지고 증강\n",
    "# overfit 대응을 위해.. l2-reg 0.1 -> 0.25 바꾸고, 입력 embeding 사이즈를 조금 줄였더니\n",
    "# 0.974222 기록 (2020.01.08 06:16) 약간 나아짐 (40번 트레이닝. 총 6시간.)\n",
    "# trainning graph에 주목.. val acc는 학습이 반복될 수록 오히려 떨어지지만, 제출점수는 높아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
